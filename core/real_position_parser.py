#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è xmlstock.com —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö
"""

import requests
import xml.etree.ElementTree as ET
import logging
import time
from urllib.parse import quote, urlparse
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)

class RealPositionParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è XMLStock API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–±–æ—Ä–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    """
    
    def __init__(self, 
                 user: str = None, 
                 key: str = None,
                 config_path: str = "config/api_keys.yaml"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        """
        self.base_url = "https://xmlstock.com/yandex/xml/"
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–∏
        if user and key:
            self.user = user
            self.key = key
        else:
            self.user, self.key = self._load_api_keys(config_path)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.settings = {
            'timeout': 30,
            'max_retries': 3,
            'retry_delay': 5,
            'region': 157,  # –ú–∏–Ω—Å–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            'max_results': 100  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        }
        
        # –ö–µ—à
        self.cache = {}
        
        logger.info(f"RealPositionParser –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. User: {self.user}")
    
    def _load_api_keys(self, config_path: str) -> Tuple[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç API –∫–ª—é—á–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑ .env
            import os
            from dotenv import load_dotenv
            load_dotenv()
            
            user = os.getenv('XMLSTOCK_USER', '8349')
            key = os.getenv('XMLSTOCK_KEY', '06ec5436d8dec99990036d862f29ea6d')
            
            return user, key
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ API –∫–ª—é—á–µ–π: {e}")
            return "8349", "06ec5436d8dec99990036d862f29ea6d"
    
    def get_search_results(self, 
                          keyword: str, 
                          region: int = None,
                          limit: int = 20) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
        
        Args:
            keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
            limit: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å (–º–∞–∫—Å 100)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        region = region or self.settings['region']
        
        logger.info(f"–ó–∞–ø—Ä–æ—Å –≤—ã–¥–∞—á–∏ –¥–ª—è '{keyword}' (—Ä–µ–≥–∏–æ–Ω: {region}, –ª–∏–º–∏—Ç: {limit})")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_encoded = quote(keyword)
        url = (f"{self.base_url}?user={self.user}&key={self.key}"
               f"&query={query_encoded}&lr={region}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(self.settings['max_retries']):
            try:
                response = requests.get(url, timeout=self.settings['timeout'])
                
                if response.status_code != 200:
                    logger.warning(f"HTTP {response.status_code} –¥–ª—è '{keyword}'")
                    if attempt < self.settings['max_retries'] - 1:
                        time.sleep(self.settings['retry_delay'])
                        continue
                    else:
                        return []
                
                # –ü–∞—Ä—Å–∏–º XML –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results = self._parse_all_search_results(response.text, limit)
                
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{keyword}': {len(results)}")
                return results
                
            except requests.exceptions.Timeout:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –¥–ª—è '{keyword}' (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                if attempt < self.settings['max_retries'] - 1:
                    time.sleep(self.settings['retry_delay'])
                    continue
                else:
                    return []
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è '{keyword}': {e}")
                return []
    
    def get_yandex_position(self, 
                           domain: str, 
                           keyword: str,
                           region: int = None,
                           include_competitors: bool = False,
                           competitors_limit: int = 20) -> Dict[str, any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é –¥–æ–º–µ–Ω–∞ –≤ –Ø–Ω–¥–µ–∫—Å + –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        
        Args:
            domain: –î–æ–º–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
            keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            include_competitors: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö
            competitors_limit: –°–∫–æ–ª—å–∫–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        region = region or self.settings['region']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_key = f"{domain}_{keyword}_{region}_{include_competitors}_{competitors_limit}"
        if cache_key in self.cache:
            logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à –¥–ª—è '{keyword}'")
            result = self.cache[cache_key].copy()
            result['cache_used'] = True
            return result
        
        logger.info(f"–ó–∞–ø—Ä–æ—Å –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è '{keyword}' (–¥–æ–º–µ–Ω: {domain}, —Ä–µ–≥–∏–æ–Ω: {region})")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        all_results = self.get_search_results(keyword, region, limit=self.settings['max_results'])
        
        # –ò—â–µ–º –Ω–∞—à –¥–æ–º–µ–Ω
        our_position = None
        our_result = None
        competitors = []
        
        for result in all_results:
            result_domain = result.get('domain', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞
            if self._domains_match(result_domain, domain):
                our_position = result.get('position')
                our_result = result
                break
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        if include_competitors and competitors_limit > 0:
            # –ë–µ—Ä—ë–º —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–∫–ª—é—á–∞—è –Ω–∞—à –¥–æ–º–µ–Ω
            for result in all_results[:competitors_limit + 10]:  # –ë–µ—Ä—ë–º —Å –∑–∞–ø–∞—Å–æ–º
                result_domain = result.get('domain', '')
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞—à –¥–æ–º–µ–Ω
                if our_result and self._domains_match(result_domain, domain):
                    continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
                competitors.append({
                    'position': result.get('position'),
                    'domain': result_domain,
                    'url': result.get('url', ''),
                    'title': result.get('title', ''),
                    'snippet': result.get('snippet', '')
                })
                
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –∫–æ–≥–¥–∞ –Ω–∞–±—Ä–∞–ª–∏ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if len(competitors) >= competitors_limit:
                    break
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "position": our_position if our_position is not None else 101,
            "url": our_result.get('url', '') if our_result else '',
            "title": our_result.get('title', '') if our_result else '',
            "domain": domain,
            "found": our_position is not None and our_position <= 100,
            "total_results": len(all_results),
            "region": region,
            "keyword": keyword,
            "timestamp": time.time()
        }
        
        if include_competitors:
            result["competitors"] = competitors
            result["top_competitors"] = competitors[:10]  # –¢–æ–ø-10 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        result['cache_used'] = False
        self.cache[cache_key] = result.copy()
        
        if our_position:
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ: '{keyword}' - –ø–æ–∑–∏—Ü–∏—è {our_position}")
        else:
            logger.info(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ: '{keyword}' - –ø–æ–∑–∏—Ü–∏—è >100")
        
        return result
    
    def batch_get_positions(self, 
                           domain: str, 
                           keywords: List[str],
                           region: int = None,
                           include_competitors: bool = False,
                           competitors_limit: int = 10) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
        
        Args:
            domain: –î–æ–º–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
            keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            region: –†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞
            include_competitors: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            competitors_limit: –õ–∏–º–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        results = []
        
        logger.info(f"–ü–∞–∫–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è {domain}. –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(keywords)}")
        
        for i, keyword in enumerate(keywords):
            try:
                logger.debug(f"–ó–∞–ø—Ä–æ—Å {i+1}/{len(keywords)}: '{keyword}'")
                
                result = self.get_yandex_position(
                    domain=domain,
                    keyword=keyword,
                    region=region,
                    include_competitors=include_competitors,
                    competitors_limit=competitors_limit
                )
                results.append(result)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(keywords) - 1:
                    time.sleep(1.5)  # 1.5 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–ª—è '{keyword}': {e}")
                results.append(self._create_error_result(str(e)))
                continue
        
        return results
    
    def _parse_all_search_results(self, xml_text: str, limit: int = 100) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏–∑ XML
        
        Args:
            xml_text: XML –æ—Ç–≤–µ—Ç –æ—Ç API
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        results = []
        
        try:
            root = ET.fromstring(xml_text)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—à–∏–±–∫–∏
            error_elem = root.find('.//error')
            if error_elem is not None:
                error_msg = error_elem.text or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ API"
                logger.error(f"–û—à–∏–±–∫–∞ API: {error_msg}")
                return results
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≥—Ä—É–ø–ø—ã (–∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ = 1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≤—ã–¥–∞—á–µ)
            groups = root.findall('.//group')
            
            for position, group in enumerate(groups, 1):
                if position > limit:
                    break
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≥—Ä—É–ø–ø—ã
                result = self._extract_result_from_group(group, position)
                if result:
                    results.append(result)
            
            logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ XML: {len(results)}")
            
        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
        
        return results
    
    def _extract_result_from_group(self, group: ET.Element, position: int) -> Optional[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –≥—Ä—É–ø–ø—ã
        
        Args:
            group: –≠–ª–µ–º–µ–Ω—Ç group –∏–∑ XML
            position: –ü–æ–∑–∏—Ü–∏—è –≤ –≤—ã–¥–∞—á–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–ª–∏ None
        """
        try:
            doc_elem = group.find('doc')
            if doc_elem is None:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
            url_elem = doc_elem.find('url')
            url = url_elem.text if url_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = doc_elem.find('title')
            title = title_elem.text if title_elem is not None else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω
            domain_elem = doc_elem.find('domain')
            domain = domain_elem.text if domain_elem is not None else ""
            
            # –ï—Å–ª–∏ –¥–æ–º–µ–Ω –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ URL
            if not domain and url:
                parsed_url = urlparse(url)
                domain = parsed_url.netloc
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–Ω–∏–ø–ø–µ—Ç
            snippet_elem = doc_elem.find('snippet')
            snippet = snippet_elem.text if snippet_elem is not None else ""
            
            # –ò—â–µ–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–º–µ–Ω–∞)
            categ_elem = group.find('categ')
            if categ_elem is not None:
                categ_domain = categ_elem.get('name', '')
                if categ_domain and not domain:
                    domain = categ_domain
            
            return {
                "position": position,
                "url": url,
                "title": title,
                "domain": domain,
                "snippet": snippet
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≥—Ä—É–ø–ø—ã: {e}")
            return None
    
    def _domains_match(self, domain1: str, domain2: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤ (—Å —É—á—ë—Ç–æ–º www –∏ –±–µ–∑)
        
        Args:
            domain1: –ü–µ—Ä–≤—ã–π –¥–æ–º–µ–Ω
            domain2: –í—Ç–æ—Ä–æ–π –¥–æ–º–µ–Ω
            
        Returns:
            True –µ—Å–ª–∏ –¥–æ–º–µ–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
        """
        if not domain1 or not domain2:
            return False
        
        # –û—á–∏—â–∞–µ–º –¥–æ–º–µ–Ω—ã
        d1 = domain1.lower().replace('www.', '').strip()
        d2 = domain2.lower().replace('www.', '').strip()
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        if d1 == d2:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤ –¥—Ä—É–≥–æ–π (–¥–ª—è –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤)
        if d1 in d2 or d2 in d1:
            logger.debug(f"–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤: '{domain1}' –∏ '{domain2}'")
            return True
        
        return False
    
    def _create_error_result(self, error_msg: str) -> Dict[str, any]:
        """–°–æ–∑–¥–∞—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π"""
        return {
            "position": None,
            "url": "",
            "title": "",
            "domain": "",
            "found": False,
            "total_results": 0,
            "region": self.settings['region'],
            "keyword": "",
            "error": error_msg,
            "timestamp": time.time()
        }


# ========== –¢–ï–°–¢–û–í–´–ô –ö–û–î ==========
def test_parser_with_competitors():
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    """
    import logging
    
    logging.basicConfig(level=logging.INFO)
    
    print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú PARSER –° –ö–û–ù–ö–£–†–ï–ù–¢–ê–ú–ò")
    print("=" * 50)
    
    try:
        # 1. –°–æ–∑–¥–∞—ë–º –ø–∞—Ä—Å–µ—Ä
        parser = RealPositionParser()
        print("‚úÖ –ü–∞—Ä—Å–µ—Ä —Å–æ–∑–¥–∞–Ω")
        
        # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã–¥–∞—á–∏
        print("\n1. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã–¥–∞—á–∏...")
        keyword = "–≤–µ–Ω–¥–∏–Ω–≥–æ–≤—ã–µ –∞–ø–ø–∞—Ä–∞—Ç—ã –∫—É–ø–∏—Ç—å"
        
        results = parser.get_search_results(keyword, limit=10)
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{keyword}': {len(results)}")
        
        if results:
            print(f"   –¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
            for i, result in enumerate(results[:5], 1):
                print(f"   {i}. [{result.get('position')}] {result.get('domain')}")
                title = result.get('title', '')
                if title:
                    print(f"      {title[:50]}...")
                else:
                    print(f"      (–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞)")
        
        # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–∏ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏
        print("\n2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–∏ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏...")
        
        result = parser.get_yandex_position(
            domain="aquamoney.by",
            keyword=keyword,
            include_competitors=True,
            competitors_limit=10
        )
        
        print(f"   –ù–∞—à–∞ –ø–æ–∑–∏—Ü–∏—è: {result.get('position')}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {result.get('found')}")
        
        competitors = result.get('competitors', [])
        print(f"   –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: {len(competitors)}")
        
        if competitors:
            print(f"\n3. –¢–æ–ø-5 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"   {i}. [{comp.get('position')}] {comp.get('domain')}")
                if comp.get('title'):
                    title = comp.get('title', '')
                    if title:
                        print(f"      {title[:40]}...")
        
        # 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        print("\n4. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å...")
        
        keywords = ["–≤–æ–¥–æ–º–∞—Ç", "–≤–µ–Ω–¥–∏–Ω–≥–æ–≤—ã–µ –∞–ø–ø–∞—Ä–∞—Ç—ã –∫—É–ø–∏—Ç—å"]
        batch_results = parser.batch_get_positions(
            domain="aquamoney.by",
            keywords=keywords,
            include_competitors=True,
            competitors_limit=5
        )
        
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(batch_results)}")
        for i, res in enumerate(batch_results):
            kw = keywords[i]
            pos = res.get('position', 'N/A')
            comp_count = len(res.get('competitors', []))
            print(f"   ‚Ä¢ '{kw}': –ø–æ–∑–∏—Ü–∏—è {pos}, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {comp_count}")
        
        print("\n" + "=" * 50)
        print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ò: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_parser_with_competitors()
